{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem Set 1 - Andreas Bloch\n",
    "This is the problem set submission of Andreas Bloch (abloch@student.ethz.ch)\n",
    "## Imports\n",
    "First we'll load some libraries and scripts that will be useful throughout the entire notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import itertools\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import swifter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading of Data\n",
    "Next we'll read the data from disk into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Data Types:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "attributes\n",
       "year                int64\n",
       "text               object\n",
       "doc                object\n",
       "reversed          float64\n",
       "num_sentences     float64\n",
       "num_words         float64\n",
       "num_letters       float64\n",
       "num_nouns         float64\n",
       "num_verbs         float64\n",
       "num_adjectives    float64\n",
       "trigrams           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define data locations\n",
    "DATA_DIR = './data'\n",
    "CASE_DIR = './data/cases'\n",
    "REVERSED_CASES = 'case_reversed.csv'\n",
    "\n",
    "# read reversal decisions from CSV\n",
    "case_reversed_csv = pd.read_csv(os.path.join(DATA_DIR, REVERSED_CASES))\n",
    "\n",
    "# create data frame to combine the data sources\n",
    "reversal_case_ids = case_reversed_csv['caseid'].values\n",
    "columns = ['year', 'text', 'doc', 'reversed',\n",
    "           'num_sentences', 'num_words', 'num_letters',\n",
    "           'num_nouns', 'num_verbs', 'num_adjectives',\n",
    "           'trigrams']\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    index=pd.Index(reversal_case_ids, name='case_id'),\n",
    "    columns=pd.Index(columns, name='attributes')\n",
    ")\n",
    "\n",
    "# store reversal decisions into data frame\n",
    "for idx, row in case_reversed_csv.iterrows():\n",
    "    case_id = row[0]\n",
    "    reversed = row[1]\n",
    "    data.at[case_id, 'reversed'] = reversed\n",
    "\n",
    "# read and store case txt files into data frame\n",
    "case_ids = []\n",
    "for file in os.listdir(os.fsencode(CASE_DIR)):\n",
    "    filename = os.fsdecode(file)\n",
    "    file_handle = open(os.path.join(CASE_DIR, filename), 'r')\n",
    "\n",
    "    year = filename.partition('_')[0]\n",
    "    case_id = filename.partition('_')[2].partition('.')[0]\n",
    "    text = file_handle.read()\n",
    "\n",
    "    case_ids.append(case_id)\n",
    "\n",
    "    data.at[case_id, 'year'] = year\n",
    "    data.at[case_id, 'text'] = text\n",
    "\n",
    "# data integrity check:\n",
    "# check that every case has a matching reversal decision and vice-versa\n",
    "if set(case_ids) != set(reversal_case_ids):\n",
    "    raise Exception('case_ids not matching! check loading of data!')\n",
    "\n",
    "# set data types\n",
    "data[['year']] = data[['year']].astype(int)\n",
    "data[['reversed']] = data[['reversed']].astype(float)\n",
    "data[['num_sentences']] = data[['num_sentences']].astype(float)\n",
    "data[['num_words']] = data[['num_words']].astype(float)\n",
    "data[['num_letters']] = data[['num_letters']].astype(float)\n",
    "data[['num_nouns']] = data[['num_nouns']].astype(float)\n",
    "data[['num_verbs']] = data[['num_verbs']].astype(float)\n",
    "data[['num_adjectives']] = data[['num_adjectives']].astype(float)\n",
    "\n",
    "# print data types\n",
    "print('Dataframe Data Types:')\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>attributes</th>\n",
       "      <th>year</th>\n",
       "      <th>text</th>\n",
       "      <th>doc</th>\n",
       "      <th>reversed</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_letters</th>\n",
       "      <th>num_nouns</th>\n",
       "      <th>num_verbs</th>\n",
       "      <th>num_adjectives</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>X3JGGO</th>\n",
       "      <td>1925</td>\n",
       "      <td>POLLOCK , District Judge.\\nFor convenience, t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3OH3J</th>\n",
       "      <td>1924</td>\n",
       "      <td>JOHNSON , Circuit Judge.\\nThis is a patent in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X3U0KO</th>\n",
       "      <td>1925</td>\n",
       "      <td>WOOLLEY , Circuit Judge.\\nThe indictment agai...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X53HAD</th>\n",
       "      <td>1924</td>\n",
       "      <td>ROGERS , Circuit Judge.\\nThe complainant is a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>X9VC5V</th>\n",
       "      <td>1925</td>\n",
       "      <td>DAWKINS , District Judge.\\nComplainant brough...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "attributes  year                                               text  doc  \\\n",
       "case_id                                                                    \n",
       "X3JGGO      1925   POLLOCK , District Judge.\\nFor convenience, t...  NaN   \n",
       "X3OH3J      1924   JOHNSON , Circuit Judge.\\nThis is a patent in...  NaN   \n",
       "X3U0KO      1925   WOOLLEY , Circuit Judge.\\nThe indictment agai...  NaN   \n",
       "X53HAD      1924   ROGERS , Circuit Judge.\\nThe complainant is a...  NaN   \n",
       "X9VC5V      1925   DAWKINS , District Judge.\\nComplainant brough...  NaN   \n",
       "\n",
       "attributes  reversed  num_sentences  num_words  num_letters  num_nouns  \\\n",
       "case_id                                                                  \n",
       "X3JGGO           0.0            NaN        NaN          NaN        NaN   \n",
       "X3OH3J           0.0            NaN        NaN          NaN        NaN   \n",
       "X3U0KO           0.0            NaN        NaN          NaN        NaN   \n",
       "X53HAD           0.0            NaN        NaN          NaN        NaN   \n",
       "X9VC5V           0.0            NaN        NaN          NaN        NaN   \n",
       "\n",
       "attributes  num_verbs  num_adjectives trigrams  \n",
       "case_id                                         \n",
       "X3JGGO            NaN             NaN      NaN  \n",
       "X3OH3J            NaN             NaN      NaN  \n",
       "X3U0KO            NaN             NaN      NaN  \n",
       "X53HAD            NaN             NaN      NaN  \n",
       "X9VC5V            NaN             NaN      NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print preview of data to show that it's loaded\n",
    "print('Dataframe Data:')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe Dimensions:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5762, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print data frame dimensions\n",
    "print('Dataframe Dimensions:')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling of Data\n",
    "\n",
    "Shuffle the data to make sure we have positive and negative reversal examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(data, random_state=23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Subsampling of Data\n",
    "\n",
    "Execute this code if you want to subsample your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSAMPLE = False         # set this to True if you want to subsample your data for performance reasons\n",
    "SUBSAMPLING_SIZE = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SUBSAMPLE: \n",
    "    data = data.head(SUBSAMPLING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "**Q:** Use spaCy to process all cases. Split the documents into sentences and tokens. Compute number of sentences, words, and letters for each document. Report histograms for these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "# do NLP with spacy on documents\n",
    "data['doc'] = data['text'].swifter.apply(nlp)\n",
    "\n",
    "# count number of sentences, words and letters\n",
    "data['num_sentences'] = data['doc'].swifter.apply(\n",
    "    lambda doc: len(list(doc.sents))\n",
    ")\n",
    "data['num_words'] = data['doc'].swifter.apply(\n",
    "    lambda doc: len(doc)\n",
    ")\n",
    "data['num_letters'] = data['text'].swifter.apply(\n",
    "    lambda text: len(''.join(filter(str.isalpha, text)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use wide figures across whole document\n",
    "sns.set(rc={'figure.figsize':(16,5)})\n",
    "\n",
    "# plot sentence frequencies\n",
    "ax = sns.distplot(data['num_sentences'].values, kde=False)\n",
    "ax.set(xlabel='Number of Sentences', ylabel='Frequency')\n",
    "plt.show()\n",
    "\n",
    "# plot word frequencies\n",
    "ax = sns.distplot(data['num_words'].values, kde=False)\n",
    "ax.set(xlabel='Number of Words', ylabel='Frequency')\n",
    "plt.show()\n",
    "\n",
    "# plot letter frequencies\n",
    "ax = sns.distplot(data['num_letters'].values, kde=False)\n",
    "ax.set(xlabel='Number of Letters', ylabel='Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "**Q:** Use the spaCy parts of speech (POS) tags to count number of nouns, verbs, and adjectives in each document. Visualize POS frequency by year.\n",
    "\n",
    "**A:** See here if you want a more detailed explanation of spacy.parts_of_speech symbols:\n",
    "\n",
    "https://universaldependencies.org/u/pos/all.html\n",
    "\n",
    "I've decided not to count proper nouns (PROPN) as nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of nouns, verbs and adjectives (through POS tags)\n",
    "data['num_nouns'] = data['doc'].swifter.apply(\n",
    "    lambda doc: sum(token.pos == spacy.parts_of_speech.NOUN for token in doc)\n",
    ")\n",
    "data['num_verbs'] = data['doc'].swifter.apply(\n",
    "    lambda doc: sum(token.pos == spacy.parts_of_speech.VERB for token in doc)\n",
    ")\n",
    "data['num_adjectives'] = data['doc'].swifter.apply(\n",
    "    lambda doc: sum(token.pos == spacy.parts_of_speech.ADJ for token in doc)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('year')['num_nouns','num_verbs','num_adjectives'].sum().plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "**Q:** Follow the steps in lecture to normalize your corpus (e.g., removing punctuation) and discuss your choices about what information to exclude. Using the normalized tokens, make a feature set of all trigrams that end in a noun.\n",
    "\n",
    "**A:** The steps are explained in the comments in the code below. The feature set will be stored in the column data['trigrams'].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jthe list of stop words used by spacy can be found in the imports above.\n",
    "# i've decided not to add or remove any stop words from this list as they\n",
    "# seem to be fine.\n",
    "print('Used Stop Words:')\n",
    "print(STOP_WORDS)\n",
    "\n",
    "# create punctuation remover\n",
    "punctuation_remover = str.maketrans('', '', punctuation)\n",
    "\n",
    "# create lemmatizer\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# create snowball stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# function to normalize a doc\n",
    "def normalize_text_and_create_trigrams(doc):\n",
    "\n",
    "    # list of found normalized 3-grams in doc\n",
    "    trigrams = []\n",
    "\n",
    "    # process sent. by sent. to avoid creating n-grams that overlap a sent.\n",
    "    for sentence in doc.sents:\n",
    "\n",
    "        words = []  # words in sentence\n",
    "        poses = []  # POSes of words in sentence\n",
    "\n",
    "        # filter out unwanted tokens and normalize the kept tokens\n",
    "        for token in sentence:\n",
    "            # get the token's word(s)\n",
    "            word = ''.join(token.text)\n",
    "            # replace newlines with spaces\n",
    "            word = word.replace('\\r', ' ').replace('\\n', ' ')\n",
    "            # remove punctuation\n",
    "            word = word.translate(punctuation_remover)\n",
    "            # replace multiple subsequent spaces with one space\n",
    "            word = re.sub(' +', ' ', word)\n",
    "            # check that word still has some text (not just one char or space)\n",
    "            if len(word) <= 1:\n",
    "                continue\n",
    "            # normalize numbers (28, 28th, 1st, ...)\n",
    "            if any(char.isdigit() for char in word):\n",
    "                word = '#'\n",
    "            # lemmatize the word\n",
    "            lemmas = lemmatizer(word, token.pos)[0]\n",
    "            if isinstance(lemmas, (list,)) and len(lemmas) > 0:\n",
    "                # pick the first option if several lemmas were found\n",
    "                word = lemmas[0]\n",
    "            else:\n",
    "                # no lemma was found (just keep the original word)\n",
    "                word = word\n",
    "            # convert the word to lowercase\n",
    "            word = word.lower()\n",
    "            # remove stopwords\n",
    "            # this check has to be done at the end because some words aren't\n",
    "            # a stop-word in their unlemmatized form. Further note that\n",
    "            # token.is_stop didn't use all the words from STOP_WORDS, so it's\n",
    "            # better to check against the full list again.\n",
    "            if word in STOP_WORDS:\n",
    "                continue\n",
    "            # stem the word to remove singular/plural\n",
    "            word = stemmer.stem(word)\n",
    "            # keep track of the token's word and pos tag\n",
    "            words.append(word)\n",
    "            poses.append(token.pos)\n",
    "\n",
    "        # zip the words and pos tags together\n",
    "        words_and_poses = list(zip(words, poses))\n",
    "\n",
    "        # function to generate list of n-grams of a sequence of items\n",
    "        def gen_n_grams(items, n):\n",
    "            if len(items) >= n:\n",
    "                for i in range(0, len(items)-n+1):\n",
    "                    yield items[i:i + n]\n",
    "\n",
    "        # generate candidate 3-grams\n",
    "        candidate_trigrams = gen_n_grams(words_and_poses, 3)\n",
    "\n",
    "        # check whether found candidate 3-grams of sentence end in a noun\n",
    "        for candidate_trigram in candidate_trigrams:\n",
    "            if candidate_trigram[2][1] == spacy.parts_of_speech.NOUN:\n",
    "                # create 3-gram string\n",
    "                trigram_string = \\\n",
    "                    candidate_trigram[0][0] + \".\" + \\\n",
    "                    candidate_trigram[1][0] + \".\" + \\\n",
    "                    candidate_trigram[2][0]\n",
    "                trigrams.append(trigram_string)\n",
    "\n",
    "    return trigrams\n",
    "\n",
    "# determine trigrams (ending in a noun) occurring in each text\n",
    "data['trigrams'] = data['doc'].swifter.apply(normalize_text_and_create_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example Trigram-Creation (Preview):')\n",
    "print('')\n",
    "print('Original Text (Preview):')\n",
    "print(data['text'].iloc[0][0:400]+\"...\")\n",
    "print('')\n",
    "print('Created Trigrams (Preview):')\n",
    "print(data['trigrams'].iloc[0][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "**Q:** Make a dataframe with at least 1000 features (frequencies over trigrams ending in a noun). Standardize the features to variance one while maintaining sparsity. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create counter to count trigram frequencies\n",
    "term_frequencies = Counter()\n",
    "\n",
    "# for each list of trigrams that appear in a case\n",
    "for case_trigrams in data['trigrams']:\n",
    "    # update the term frequencies\n",
    "    term_frequencies.update(case_trigrams)\n",
    "\n",
    "# print 100 most common trigrams\n",
    "print('100 Most Common Trigrams:')\n",
    "for term_freq in term_frequencies.most_common(100):\n",
    "    print(term_freq)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary (set of trigrams) from the 1000 most common trigrams\n",
    "vocab = set(x[0] for x in term_frequencies.most_common(1000))\n",
    "\n",
    "# create data frame to represent cases with features according to 1000 most common trigrams\n",
    "data_featurized_columns = list(vocab)+['reversed']\n",
    "data_featurized = pd.DataFrame(\n",
    "    index=pd.Index(list(data.index.values), name='case_id'),\n",
    "    columns=pd.Index(data_featurized_columns, name='features')\n",
    ")\n",
    "\n",
    "# print shape of featurized data frame\n",
    "print('Featurized Data Shape:')\n",
    "data_featurized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each list of trigrams that appear in a case, create its vector representation\n",
    "# according to the 1000 most common trigrams\n",
    "for case_id, case in data.iterrows():\n",
    "    # create dictionary to count trigram occurrences\n",
    "    # (of trigrams from 1000 most common trigrams)\n",
    "    # initialize all counts with zero\n",
    "    case_trigram_features = dict((t,0) for t in vocab)\n",
    "    # for each trigram appearing in the case\n",
    "    for trigram in case['trigrams']:\n",
    "        # if the trigram appears in the 1000 most common trigrams vocabulary\n",
    "        if trigram in vocab:\n",
    "            # increment the count for that trigram\n",
    "            case_trigram_features[trigram] += 1\n",
    "    # keep track of label (whether the case was reversed)\n",
    "    case_trigram_features['reversed'] = case['reversed']\n",
    "    # store the case_trigram_counts and reversal decision into data_featurized\n",
    "    for col, val in case_trigram_features.items():\n",
    "        data_featurized.at[str(case_id), str(col)] = float(val)\n",
    "\n",
    "# print a preview of the data features\n",
    "print('Data 1000-Most-Common-Trigrams-Featurized:')\n",
    "data_featurized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data matrices\n",
    "X = data_featurized.values.astype(float)\n",
    "y = X[:,-1]\n",
    "X = X[:,:-1]\n",
    "\n",
    "# print shapes of data matrices\n",
    "print('Shapes of X and y:')\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# print preview of X\n",
    "print('X= (preview)')\n",
    "X[0:10,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize features (without subtracting mean to maintain sparsity)\n",
    "scaler = StandardScaler(copy=False, with_mean=False, with_std=True)\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# print rescaled data\n",
    "print('X= (preview)')\n",
    "X[0:10,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "**Q:** Link the dataframe to the outcome reverse. Create a training set and test set. Train a LogisticRegression model with default parameters to predict reversal. Compute accuracy and F1 for the prediction in the training set and in the test set. \n",
    "\n",
    "**A:** The cases and the reversal outcomes have already been linked in the data loading stage (at the beginning of this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data (again just to make sure)\n",
    "X, y = shuffle(X, y, random_state=71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# train a LogisticRegression model with the default parameters\n",
    "# (specify solver to avoid warnings)\n",
    "log_reg = LogisticRegression(solver='liblinear')\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# do predictions on training and test set\n",
    "y_train_pred = log_reg.predict(X_train)\n",
    "y_test_pred = log_reg.predict(X_test)\n",
    "\n",
    "# print accuracy and F1 for training set\n",
    "acc = accuracy_score(y_train, y_train_pred, normalize=True)\n",
    "f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "print('Accuracy and F1-Score on Training Set with Default Parameters:')\n",
    "print('Accuracy: \\t'+str(acc))\n",
    "print('F1: \\t\\t'+str(f1))\n",
    "print('')\n",
    "\n",
    "# print accuracy and F1 for test set\n",
    "acc = accuracy_score(y_test, y_test_pred, normalize=True)\n",
    "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
    "print('Accuracy and F1-Score on Test Set with Default Parameters:')\n",
    "print('Accuracy: \\t'+str(acc))\n",
    "print('F1: \\t\\t'+str(f1))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n",
    "\n",
    "**Q:** Use GridSearchCV() to choose hyperparameters: L1 vs L2 penalty, and regularization parameter C. Report the best model parameters and score. Report the ROC Curve and AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logistic regression model\n",
    "# (specify solver to avoid warnings)\n",
    "# (increase max_iter to ensure convergence)\n",
    "log_reg = LogisticRegression(\n",
    "    solver='liblinear',\n",
    "    max_iter=10000\n",
    ")\n",
    "\n",
    "# specify parameter grid\n",
    "# - alpha: constant that multiplies L1/L2-penalty terms\n",
    "# - l1_ratio: mixing parameter\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 0.5, 0.7, 1.0, 1.5, 2.0, 5.0]\n",
    "}\n",
    "\n",
    "# specify grid search\n",
    "grid = GridSearchCV(\n",
    "    estimator=log_reg,          # estimator to use\n",
    "    param_grid=param_grid,      # parameters to do grid search over\n",
    "    scoring='f1',               # use F1 score to evaluate models\n",
    "    n_jobs=-1,                  # use all cores\n",
    "    iid=True,                   # assume data was i.i.d. (to avoid warning)\n",
    "    cv=10,                      # use stratified 10-fold CV\n",
    "    refit=True,                 # re-fit best model\n",
    "    verbose=0,                  # do not print training progress\n",
    "    return_train_score=True     # save training scores\n",
    ")\n",
    "\n",
    "# train with grid-search\n",
    "grid.fit(X, y)\n",
    "print('')\n",
    "\n",
    "# report best hyperparameters\n",
    "print('Best Hyperparameters:')\n",
    "print(grid.best_params_)\n",
    "print('')\n",
    "\n",
    "# report the best score\n",
    "print('Best Score:')\n",
    "print(grid.best_score_)\n",
    "print('')\n",
    "\n",
    "# keep track of best model\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "# print ROC curve\n",
    "y_pred = cross_val_predict(best_model, X, y, method='decision_function', cv=10)\n",
    "fpr, tpr, thresholds = roc_curve(y, y_pred)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.show()\n",
    "\n",
    "# print area under the curve (AUC) score\n",
    "auc = roc_auc_score(y, y_pred)\n",
    "print('Area under the Curve (AUC) of Best Model:')\n",
    "print('AUC: '+str(auc))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7\n",
    "\n",
    "**Q:** Make a new dataframe where each sentence (of each case) is treated as a separate document. Compute vader compound sentiment scores for each sentence and report the top 10 highest- and lowest-sentiment sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build data frame for sentences\n",
    "columns = [\n",
    "    'sentence',\n",
    "    'sentiment_score',\n",
    "    'cluster'\n",
    "]\n",
    "sent_data = pd.DataFrame(columns=pd.Index(columns, name='attributes'))\n",
    "\n",
    "# set data frame data types\n",
    "sent_data[['sentiment_score']] = sent_data[['sentiment_score']].astype(float)\n",
    "sent_data[['cluster']] = sent_data[['cluster']].astype(int)\n",
    "\n",
    "# gets sentences of a document\n",
    "def get_sentences(doc):\n",
    "    sentences = []\n",
    "    for sentence in doc.sents:\n",
    "        # replace newlines with spaces\n",
    "        sentence = sentence.text.replace('\\r', ' ').replace('\\n', ' ')\n",
    "        sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "# extract sentences from previously processed data\n",
    "sentences = data['doc'].swifter.apply(get_sentences)\n",
    "\n",
    "# store sentences into data frame\n",
    "sent_data['sentence'] = list(itertools.chain.from_iterable(sentences.values))\n",
    "\n",
    "# print data types\n",
    "print('Dataframe Data Types:')\n",
    "print(sent_data.dtypes)\n",
    "print('')\n",
    "\n",
    "# print preview of data to show that it's loaded\n",
    "print('Dataframe Data:')\n",
    "print(sent_data.head())\n",
    "print('')\n",
    "\n",
    "# create sentiment intensity analyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# get the compound polarity score for a sentence\n",
    "def get_compound_polarity_score(sentence):\n",
    "    scores = sid.polarity_scores(sentence)\n",
    "    compound_score = scores['compound']\n",
    "    return compound_score\n",
    "\n",
    "# compute the sentiment score for all sentences in data\n",
    "sent_data['sentiment_score'] = \\\n",
    "    sent_data['sentence'].swifter.apply(get_compound_polarity_score)\n",
    "\n",
    "# sort sentences by sentiment score\n",
    "sent_data = sent_data.sort_values(by=['sentiment_score'], ascending=False)\n",
    "\n",
    "def print_sentences(sents):\n",
    "    for idx, row in sents.iterrows():\n",
    "        print('(Score: '+str(row['sentiment_score'])+') '+str(row['sentence']))\n",
    "        print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 Highest Sentiment Sentences:\\n')\n",
    "print_sentences(sent_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('10 Lowest Sentiment Sentences:\\n')\n",
    "print_sentences(sent_data.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-sort data according to index\n",
    "sent_data = sent_data.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8\n",
    "\n",
    "**Q:** Use TfidfVectorizer to compute tf-idf frequencies for each sentence, and then compute cosine similarities between all sentences. Report example pairs of very similar sentences and very dissimilar sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the collection of raw sentences to a matrix of TF-IDF features\n",
    "tfidf = TfidfVectorizer(\n",
    "    strip_accents='ascii',  # only consider ascii characters\n",
    "    lowercase=True,         # convert text to lowercase\n",
    "    stop_words='english',   # use english stop words\n",
    "    ngram_range=(2, 3),     # consider (2 to 3)-grams for embeddings\n",
    "    max_df=0.5,             # ignore terms that are in more than 50% of docs\n",
    "    min_df=0.001,           # ignore terms that are in less than .1% of docs\n",
    "    max_features=1000,      # only consider top 1000 terms (w.r.t. term freq.)\n",
    "    binary=False,           # do not use binary counts (use integer counts)\n",
    "    norm='l2',              # each sentence-embedding will have unit-norm\n",
    "    use_idf=True,           # enables inverse-doc-freq weighting\n",
    ")\n",
    "\n",
    "# fit TF-IDF Vectorizer\n",
    "X = tfidf.fit_transform(sent_data['sentence'])\n",
    "\n",
    "# print embedding dimensions\n",
    "print('Embedding Dimensionality Chosen by TfidfVectorizer: '+str(X.shape[1]))\n",
    "\n",
    "# compute the pairwise cosine similarirties\n",
    "S = cosine_similarity(X, X)\n",
    "\n",
    "def largest_indices(arr, n):\n",
    "    \"\"\"Returns the n largest indices from a numpy array.\"\"\"\n",
    "    flat = arr.flatten()\n",
    "    indices = np.argpartition(flat, -n)[-n:]\n",
    "    indices = indices[np.argsort(-flat[indices])]\n",
    "    return np.unravel_index(indices, arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of S for efficiency reasons\n",
    "max_idx = min(1000, S.shape[0])\n",
    "S = S[0:max_idx,0:max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report an example of very similar sentences\n",
    "most_sim_idx = \\\n",
    "    np.unravel_index(np.argmax(S, axis=None), S.shape)\n",
    "print('Two very similar sentences are ' + str(most_sim_idx) +\n",
    "      ' with similarity '+str(S[most_sim_idx])+'\\n')\n",
    "print(str(most_sim_idx[0])+\": \"+sent_data.iloc[most_sim_idx[0]]['sentence']+'\\n')\n",
    "print(str(most_sim_idx[1])+\": \"+sent_data.iloc[most_sim_idx[1]]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report an example of very dissimilar sentences\n",
    "most_dissim_idx = np.unravel_index(np.argmin(S, axis=None), S.shape)\n",
    "print('Two very dissimilar sentences are ' + str(most_dissim_idx) +\n",
    "      ' with similarity '+str(S[most_dissim_idx])+\":\"+'\\n')\n",
    "print(str(most_dissim_idx[0])+\": \"+sent_data.iloc[most_dissim_idx[0]]['sentence']+'\\n')\n",
    "print(str(most_dissim_idx[1])+\": \"+sent_data.iloc[most_dissim_idx[1]]['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9\n",
    "\n",
    "**Q:** Use k-means clustering to assign the sentences into 20 clusters. List 5 example sentences from each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build k-means model (with k=20)\n",
    "num_clusters = 20\n",
    "km = KMeans(n_clusters=num_clusters, n_jobs=-1)\n",
    "\n",
    "# fit k-means model to sentences\n",
    "km.fit(X)\n",
    "\n",
    "# get the cluster centers\n",
    "V = km.cluster_centers_\n",
    "\n",
    "# add cluster labels to data\n",
    "cluster_labels = km.labels_.tolist()\n",
    "sent_data['cluster'] = cluster_labels\n",
    "\n",
    "# list 5 sentences from each cluster\n",
    "for cluster_idx in range(0, 20):\n",
    "    cluster_members = sent_data[sent_data['cluster'] == cluster_idx]\n",
    "    print('Members of Cluster '+str(cluster_idx)+':')\n",
    "    for idx, row in cluster_members.head(5).iterrows():\n",
    "        print(row['sentence'])\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
